{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 8241,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.03641329085116067,
      "grad_norm": 1.3758982419967651,
      "learning_rate": 0.00019800000000000002,
      "loss": 8.1743,
      "step": 100
    },
    {
      "epoch": 0.07282658170232134,
      "grad_norm": 1.494292974472046,
      "learning_rate": 0.00019756786635548459,
      "loss": 5.2411,
      "step": 200
    },
    {
      "epoch": 0.10923987255348203,
      "grad_norm": 1.781347632408142,
      "learning_rate": 0.00019516029971747945,
      "loss": 4.753,
      "step": 300
    },
    {
      "epoch": 0.14565316340464268,
      "grad_norm": 1.8619987964630127,
      "learning_rate": 0.00019275273307947427,
      "loss": 4.5735,
      "step": 400
    },
    {
      "epoch": 0.18206645425580337,
      "grad_norm": 2.3369834423065186,
      "learning_rate": 0.0001902960324284486,
      "loss": 4.3829,
      "step": 500
    },
    {
      "epoch": 0.21847974510696405,
      "grad_norm": 1.813658595085144,
      "learning_rate": 0.00018783933177742292,
      "loss": 4.2168,
      "step": 600
    },
    {
      "epoch": 0.25489303595812474,
      "grad_norm": 1.9636729955673218,
      "learning_rate": 0.00018538263112639726,
      "loss": 4.0733,
      "step": 700
    },
    {
      "epoch": 0.29130632680928537,
      "grad_norm": 2.2239291667938232,
      "learning_rate": 0.0001829259304753716,
      "loss": 4.0326,
      "step": 800
    },
    {
      "epoch": 0.32771961766044605,
      "grad_norm": 1.853506326675415,
      "learning_rate": 0.0001804692298243459,
      "loss": 4.015,
      "step": 900
    },
    {
      "epoch": 0.36413290851160673,
      "grad_norm": 2.5727956295013428,
      "learning_rate": 0.00017801252917332022,
      "loss": 3.8807,
      "step": 1000
    },
    {
      "epoch": 0.4005461993627674,
      "grad_norm": 1.772547721862793,
      "learning_rate": 0.00017555582852229456,
      "loss": 3.9279,
      "step": 1100
    },
    {
      "epoch": 0.4369594902139281,
      "grad_norm": 1.934130311012268,
      "learning_rate": 0.0001730991278712689,
      "loss": 3.8823,
      "step": 1200
    },
    {
      "epoch": 0.47337278106508873,
      "grad_norm": 2.144239664077759,
      "learning_rate": 0.00017064242722024321,
      "loss": 3.8708,
      "step": 1300
    },
    {
      "epoch": 0.5097860719162495,
      "grad_norm": 2.1479036808013916,
      "learning_rate": 0.00016818572656921753,
      "loss": 3.8372,
      "step": 1400
    },
    {
      "epoch": 0.5461993627674101,
      "grad_norm": 2.70529842376709,
      "learning_rate": 0.0001657290259181919,
      "loss": 3.7753,
      "step": 1500
    },
    {
      "epoch": 0.5826126536185707,
      "grad_norm": 2.1320230960845947,
      "learning_rate": 0.0001632723252671662,
      "loss": 3.7931,
      "step": 1600
    },
    {
      "epoch": 0.6190259444697315,
      "grad_norm": 3.0089809894561768,
      "learning_rate": 0.00016081562461614052,
      "loss": 3.7383,
      "step": 1700
    },
    {
      "epoch": 0.6554392353208921,
      "grad_norm": 2.2227060794830322,
      "learning_rate": 0.00015835892396511486,
      "loss": 3.7546,
      "step": 1800
    },
    {
      "epoch": 0.6918525261720528,
      "grad_norm": 2.4602863788604736,
      "learning_rate": 0.0001559022233140892,
      "loss": 3.7273,
      "step": 1900
    },
    {
      "epoch": 0.7282658170232135,
      "grad_norm": 2.304202079772949,
      "learning_rate": 0.0001534455226630635,
      "loss": 3.7391,
      "step": 2000
    },
    {
      "epoch": 0.7646791078743741,
      "grad_norm": 2.0634169578552246,
      "learning_rate": 0.00015098882201203782,
      "loss": 3.7175,
      "step": 2100
    },
    {
      "epoch": 0.8010923987255348,
      "grad_norm": 2.3423912525177,
      "learning_rate": 0.0001485321213610122,
      "loss": 3.7043,
      "step": 2200
    },
    {
      "epoch": 0.8375056895766955,
      "grad_norm": 2.4321751594543457,
      "learning_rate": 0.0001460754207099865,
      "loss": 3.6946,
      "step": 2300
    },
    {
      "epoch": 0.8739189804278562,
      "grad_norm": 2.2106540203094482,
      "learning_rate": 0.00014361872005896081,
      "loss": 3.6894,
      "step": 2400
    },
    {
      "epoch": 0.9103322712790168,
      "grad_norm": 3.111199378967285,
      "learning_rate": 0.00014116201940793515,
      "loss": 3.6202,
      "step": 2500
    },
    {
      "epoch": 0.9467455621301775,
      "grad_norm": 2.438849449157715,
      "learning_rate": 0.0001387053187569095,
      "loss": 3.7296,
      "step": 2600
    },
    {
      "epoch": 0.9831588529813382,
      "grad_norm": 2.3104846477508545,
      "learning_rate": 0.0001362486181058838,
      "loss": 3.6654,
      "step": 2700
    },
    {
      "epoch": 1.0192990441511152,
      "grad_norm": 2.76583194732666,
      "learning_rate": 0.00013381648446136838,
      "loss": 3.6386,
      "step": 2800
    },
    {
      "epoch": 1.055712335002276,
      "grad_norm": 2.1756603717803955,
      "learning_rate": 0.0001313597838103427,
      "loss": 3.6288,
      "step": 2900
    },
    {
      "epoch": 1.0921256258534364,
      "grad_norm": 2.0680747032165527,
      "learning_rate": 0.00012890308315931706,
      "loss": 3.5645,
      "step": 3000
    },
    {
      "epoch": 1.1285389167045972,
      "grad_norm": 2.56768798828125,
      "learning_rate": 0.00012644638250829137,
      "loss": 3.6726,
      "step": 3100
    },
    {
      "epoch": 1.164952207555758,
      "grad_norm": 2.5141685009002686,
      "learning_rate": 0.00012398968185726569,
      "loss": 3.6094,
      "step": 3200
    },
    {
      "epoch": 1.2013654984069184,
      "grad_norm": 2.8073439598083496,
      "learning_rate": 0.00012153298120624001,
      "loss": 3.654,
      "step": 3300
    },
    {
      "epoch": 1.2377787892580792,
      "grad_norm": 3.382990837097168,
      "learning_rate": 0.00011907628055521436,
      "loss": 3.572,
      "step": 3400
    },
    {
      "epoch": 1.27419208010924,
      "grad_norm": 2.6431374549865723,
      "learning_rate": 0.00011661957990418868,
      "loss": 3.6275,
      "step": 3500
    },
    {
      "epoch": 1.3106053709604004,
      "grad_norm": 2.481748580932617,
      "learning_rate": 0.000114162879253163,
      "loss": 3.5668,
      "step": 3600
    },
    {
      "epoch": 1.3470186618115612,
      "grad_norm": 2.486811637878418,
      "learning_rate": 0.00011170617860213734,
      "loss": 3.5404,
      "step": 3700
    },
    {
      "epoch": 1.383431952662722,
      "grad_norm": 2.6303961277008057,
      "learning_rate": 0.00010924947795111167,
      "loss": 3.5943,
      "step": 3800
    },
    {
      "epoch": 1.4198452435138826,
      "grad_norm": 2.783478260040283,
      "learning_rate": 0.000106792777300086,
      "loss": 3.6508,
      "step": 3900
    },
    {
      "epoch": 1.4562585343650434,
      "grad_norm": 2.644380569458008,
      "learning_rate": 0.00010433607664906031,
      "loss": 3.5743,
      "step": 4000
    },
    {
      "epoch": 1.492671825216204,
      "grad_norm": 2.7415595054626465,
      "learning_rate": 0.00010187937599803466,
      "loss": 3.5368,
      "step": 4100
    },
    {
      "epoch": 1.5290851160673646,
      "grad_norm": 2.398012399673462,
      "learning_rate": 9.942267534700897e-05,
      "loss": 3.5243,
      "step": 4200
    },
    {
      "epoch": 1.5654984069185254,
      "grad_norm": 2.5016965866088867,
      "learning_rate": 9.69659746959833e-05,
      "loss": 3.5422,
      "step": 4300
    },
    {
      "epoch": 1.601911697769686,
      "grad_norm": 2.5790441036224365,
      "learning_rate": 9.450927404495762e-05,
      "loss": 3.5008,
      "step": 4400
    },
    {
      "epoch": 1.6383249886208466,
      "grad_norm": 2.838254690170288,
      "learning_rate": 9.205257339393195e-05,
      "loss": 3.554,
      "step": 4500
    },
    {
      "epoch": 1.6747382794720074,
      "grad_norm": 2.9538989067077637,
      "learning_rate": 8.959587274290629e-05,
      "loss": 3.6146,
      "step": 4600
    },
    {
      "epoch": 1.711151570323168,
      "grad_norm": 2.827307939529419,
      "learning_rate": 8.71391720918806e-05,
      "loss": 3.5547,
      "step": 4700
    },
    {
      "epoch": 1.7475648611743286,
      "grad_norm": 2.771329402923584,
      "learning_rate": 8.468247144085494e-05,
      "loss": 3.5134,
      "step": 4800
    },
    {
      "epoch": 1.7839781520254894,
      "grad_norm": 3.552834987640381,
      "learning_rate": 8.222577078982925e-05,
      "loss": 3.5889,
      "step": 4900
    },
    {
      "epoch": 1.82039144287665,
      "grad_norm": 2.572551727294922,
      "learning_rate": 7.97690701388036e-05,
      "loss": 3.5769,
      "step": 5000
    },
    {
      "epoch": 1.8568047337278106,
      "grad_norm": 2.986523151397705,
      "learning_rate": 7.731236948777792e-05,
      "loss": 3.5299,
      "step": 5100
    },
    {
      "epoch": 1.8932180245789714,
      "grad_norm": 2.376110315322876,
      "learning_rate": 7.485566883675225e-05,
      "loss": 3.5511,
      "step": 5200
    },
    {
      "epoch": 1.9296313154301319,
      "grad_norm": 2.839658498764038,
      "learning_rate": 7.239896818572657e-05,
      "loss": 3.5345,
      "step": 5300
    },
    {
      "epoch": 1.9660446062812926,
      "grad_norm": 2.9411566257476807,
      "learning_rate": 6.99422675347009e-05,
      "loss": 3.5304,
      "step": 5400
    },
    {
      "epoch": 2.00218479745107,
      "grad_norm": 3.9167416095733643,
      "learning_rate": 6.748556688367522e-05,
      "loss": 3.5293,
      "step": 5500
    },
    {
      "epoch": 2.0385980883022303,
      "grad_norm": 2.493703603744507,
      "learning_rate": 6.502886623264955e-05,
      "loss": 3.5169,
      "step": 5600
    },
    {
      "epoch": 2.075011379153391,
      "grad_norm": 3.1526288986206055,
      "learning_rate": 6.257216558162389e-05,
      "loss": 3.4766,
      "step": 5700
    },
    {
      "epoch": 2.111424670004552,
      "grad_norm": 2.6669704914093018,
      "learning_rate": 6.011546493059821e-05,
      "loss": 3.5247,
      "step": 5800
    },
    {
      "epoch": 2.1478379608557123,
      "grad_norm": 3.289682149887085,
      "learning_rate": 5.765876427957254e-05,
      "loss": 3.5058,
      "step": 5900
    },
    {
      "epoch": 2.184251251706873,
      "grad_norm": 3.085087299346924,
      "learning_rate": 5.520206362854686e-05,
      "loss": 3.481,
      "step": 6000
    },
    {
      "epoch": 2.220664542558034,
      "grad_norm": 2.6984496116638184,
      "learning_rate": 5.274536297752119e-05,
      "loss": 3.4881,
      "step": 6100
    },
    {
      "epoch": 2.2570778334091943,
      "grad_norm": 2.98917555809021,
      "learning_rate": 5.028866232649552e-05,
      "loss": 3.48,
      "step": 6200
    },
    {
      "epoch": 2.293491124260355,
      "grad_norm": 2.639381170272827,
      "learning_rate": 4.7831961675469845e-05,
      "loss": 3.563,
      "step": 6300
    },
    {
      "epoch": 2.329904415111516,
      "grad_norm": 3.1447322368621826,
      "learning_rate": 4.537526102444417e-05,
      "loss": 3.4855,
      "step": 6400
    },
    {
      "epoch": 2.3663177059626763,
      "grad_norm": 2.9272656440734863,
      "learning_rate": 4.29185603734185e-05,
      "loss": 3.4454,
      "step": 6500
    },
    {
      "epoch": 2.402730996813837,
      "grad_norm": 2.9893898963928223,
      "learning_rate": 4.046185972239283e-05,
      "loss": 3.4369,
      "step": 6600
    },
    {
      "epoch": 2.439144287664998,
      "grad_norm": 3.061716079711914,
      "learning_rate": 3.8005159071367155e-05,
      "loss": 3.5313,
      "step": 6700
    },
    {
      "epoch": 2.4755575785161583,
      "grad_norm": 2.951676845550537,
      "learning_rate": 3.554845842034149e-05,
      "loss": 3.4295,
      "step": 6800
    },
    {
      "epoch": 2.5119708693673193,
      "grad_norm": 2.5698063373565674,
      "learning_rate": 3.3091757769315814e-05,
      "loss": 3.513,
      "step": 6900
    },
    {
      "epoch": 2.54838416021848,
      "grad_norm": 3.0677218437194824,
      "learning_rate": 3.063505711829014e-05,
      "loss": 3.4611,
      "step": 7000
    },
    {
      "epoch": 2.5847974510696403,
      "grad_norm": 3.0457332134246826,
      "learning_rate": 2.8178356467264466e-05,
      "loss": 3.4635,
      "step": 7100
    },
    {
      "epoch": 2.621210741920801,
      "grad_norm": 3.184699773788452,
      "learning_rate": 2.572165581623879e-05,
      "loss": 3.499,
      "step": 7200
    },
    {
      "epoch": 2.657624032771962,
      "grad_norm": 3.0118768215179443,
      "learning_rate": 2.326495516521312e-05,
      "loss": 3.4978,
      "step": 7300
    },
    {
      "epoch": 2.6940373236231223,
      "grad_norm": 2.7183475494384766,
      "learning_rate": 2.0808254514187447e-05,
      "loss": 3.4959,
      "step": 7400
    },
    {
      "epoch": 2.7304506144742833,
      "grad_norm": 2.4992358684539795,
      "learning_rate": 1.8351553863161773e-05,
      "loss": 3.4882,
      "step": 7500
    },
    {
      "epoch": 2.766863905325444,
      "grad_norm": 2.8151609897613525,
      "learning_rate": 1.5894853212136102e-05,
      "loss": 3.4668,
      "step": 7600
    },
    {
      "epoch": 2.8032771961766043,
      "grad_norm": 2.7517223358154297,
      "learning_rate": 1.3438152561110428e-05,
      "loss": 3.4892,
      "step": 7700
    },
    {
      "epoch": 2.8396904870277653,
      "grad_norm": 2.8243649005889893,
      "learning_rate": 1.0981451910084758e-05,
      "loss": 3.4395,
      "step": 7800
    },
    {
      "epoch": 2.876103777878926,
      "grad_norm": 2.6439075469970703,
      "learning_rate": 8.524751259059084e-06,
      "loss": 3.4549,
      "step": 7900
    },
    {
      "epoch": 2.9125170687300868,
      "grad_norm": 2.825427532196045,
      "learning_rate": 6.068050608033411e-06,
      "loss": 3.4488,
      "step": 8000
    },
    {
      "epoch": 2.9489303595812473,
      "grad_norm": 2.374843120574951,
      "learning_rate": 3.611349957007739e-06,
      "loss": 3.457,
      "step": 8100
    },
    {
      "epoch": 2.985343650432408,
      "grad_norm": 3.345824718475342,
      "learning_rate": 1.154649305982066e-06,
      "loss": 3.4411,
      "step": 8200
    }
  ],
  "logging_steps": 100,
  "max_steps": 8241,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.068494800224256e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
